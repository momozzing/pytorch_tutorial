{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "import torchtext\n",
    "from konlpy.tag import Mecab\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Mecab()\n",
    "\n",
    "ID = torchtext.data.Field(sequential = False,\n",
    "                use_vocab = False) # 실제 사용은 하지 않을 예정\n",
    "\n",
    "TEXT = torchtext.data.Field(sequential=True,\n",
    "                  use_vocab=True,\n",
    "                  tokenize=tokenizer.morphs, # 토크나이저로는 Mecab 사용.\n",
    "                  lower=True,\n",
    "                  include_lengths = True,\n",
    "                  fix_length = 64,\n",
    "                  batch_first= True)\n",
    "\n",
    "LABEL = torchtext.data.Field(sequential=False,\n",
    "                   use_vocab=False,\n",
    "                   is_target=True,\n",
    "                   dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data.TabularDataset.splits(\n",
    "        path='./data', train='ratings_train.txt', test='ratings_test.txt', format='tsv',\n",
    "        fields=[('id', ID), ('text', TEXT), ('label', LABEL)], skip_header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torchtext.data.dataset.TabularDataset at 0x7fc56572c0d0>"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'id': '6777299', 'text': ['무조건', '봐야지'], 'label': '1'}\n{'id': '6495545', 'text': ['유쾌', '한', '영화', '에', '요', '아줌마', '가', '공감', '가', '는', '부분', '도', '많', '고', '사라제시카파커', '를', '좋아해서', '더욱', '좋', '았', '던', '영화', '입니다'], 'label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data[0]))\n",
    "print(vars(train_data[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "단어 집합의 크기 : 25002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('단어 집합의 크기 : {}'.format(len(TEXT.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "단어 집합의 크기 : 3\n"
     ]
    }
   ],
   "source": [
    "print('단어 집합의 크기 : {}'.format(len(LABEL.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7fc478967410>>, {'<unk>': 0, '0': 1, '1': 2})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "': 23536, '작의': 23537, '잔다는': 23538, '잔득': 23539, '잔인성': 23540, '잔적': 23541, '잔혹성': 23542, '잖여': 23543, '잘나': 23544, '잘랐': 23545, '잘려나갔': 23546, '잘만킹': 23547, '잠겼': 23548, '잠그': 23549, '잠긴다': 23550, '잠길': 23551, '잠수': 23552, '잠수종': 23553, '잠재력': 23554, '잠적': 23555, '잡것': 23556, '잡숴': 23557, '잡스런': 23558, '잡스럽': 23559, '잡식': 23560, '잡아가': 23561, '잡영': 23562, '잡혀가': 23563, '잡힌다': 23564, '장갑': 23565, '장거리': 23566, '장금': 23567, '장끌로드': 23568, '장난삼': 23569, '장님': 23570, '장동혁': 23571, '장량': 23572, '장렬': 23573, '장례식장': 23574, '장마': 23575, '장만': 23576, '장모님': 23577, '장미꽃': 23578, '장민': 23579, '장민경': 23580, '장발': 23581, '장백': 23582, '장병': 23583, '장수원': 23584, '장악': 23585, '장위안': 23586, '장의사': 23587, '장인어른': 23588, '장일산': 23589, '장임': 23590, '장자': 23591, '장자연': 23592, '장전': 23593, '장조': 23594, '장창': 23595, '장태정': 23596, '장하': 23597, '장항': 23598, '장항준': 23599, '장현수': 23600, '장훈': 23601, '재감': 23602, '재고': 23603, '재능기부': 23604, '재량': 23605, '재무': 23606, '재미난다': 23607, '재밌기만': 23608, '재스민': 23609, '재인': 23610, '재작년': 23611, '재중': 23612, '재클린': 23613, '재탄생': 23614, '재환': 23615, '재활': 23616, '잭바우어': 23617, '쟁': 23618, '쟎아요': 23619, '저글링': 23620, '저금': 23621, '저기요': 23622, '저널리스트': 23623, '저래요': 23624, '저리도': 23625, '저림': 23626, '저릿저릿': 23627, '저마다': 23628, '저메키스': 23629, '저물': 23630, '저변': 23631, '저비': 23632, '저스티스': 23633, '저우쉰': 23634, '저인': 23635, '저지를': 23636, '저질르': 23637, '저쨌': 23638, '저택': 23639, '저편': 23640, '적격': 23641, '적군파': 23642, '적그리스도': 23643, '적반하장': 23644, '적발': 23645, '적선': 23646, '적우': 23647, '적임자': 23648, '적잖이': 23649, '적정선': 23650, '전구': 23651, '전날': 23652, '전남': 23653, '전념': 23654, '전뇌': 23655, '전대미문': 23656, '전래': 23657, '전례': 23658, '전매특허': 23659, '전물': 23660, '전방': 23661, '전봇대': 23662, '전북': 23663, '전분': 23664, '전소민': 23665, '전액': 23666, '전염병': 23667, '전영화': 23668, '전유물': 23669, '전윤수': 23670, '전재': 23671, '전재홍': 23672, '전제': 23673, '전지': 23674, '전처': 23675, '전철': 23676, '전체주의': 23677, '전통문화': 23678, '전학': 23679, '전한다': 23680, '전할': 23681, '전함': 23682, '전해져온다': 23683, '전해졌': 23684, '전해짐': 23685, '전현': 23686, '전화선': 23687, '전회': 23688, '전효정': 23689, '절름발': 23690, '절름발이': 23691, '절봉': 23692, '절세': 23693, '절인': 23694, '절정기': 23695, '절제미': 23696, '절판': 23697, '점감': 23698, '점장': 23699, '점조': 23700, '접니다': 23701, '접시': 23702, '접어들': 23703, '접점': 23704, '접접': 23705, '접히': 23706, '젓개': 23707, '정경': 23708, '정경애': 23709, '정난정': 23710, '정당방위': 23711, '정도전': 23712, '정떨어질': 23713, '정말루': 23714, '정미': 23715, '정민': 23716, '정밀': 23717, '정반': 23718, '정벌': 23719, '정복자': 23720, '정비': 23721, '정세': 23722, '정소영': 23723, '정수문': 23724, '정승필': 23725, '정신과': 23726, '정신력': 23727, '정완': 23728, '정용': 23729, '정유경': 23730, '정윤': 23731, '정은우': 23732, '정은표': 23733, '정일성': 23734, '정재': 23735, '정창화': 23736, '정체기': 23737, '정치성향': 23738, '정크푸드': 23739, '정태': 23740, '정태원': 23741, '정해서': 23742, '정해야': 23743, '정훈': 23744, '젖몸살': 23745, '젖어들': 23746, '젖통': 23747, '제각각': 23748, '제갈': 23749, '제곱': 23750, '제기럴': 23751, '제꼈': 23752, '제네시스': 23753, '제때': 23754, '제레미섬터': 23755, '제로니모': 23756, '제로인': 23757, '제리케이': 23758, '제멋': 23759, '제몫을': 23760, '제보': 23761, '제복': 23762, '제빵': 23763, '제사': 23764, '제스': 23765, '제시간': 23766, '제시카랭': 23767, '제안': 23768, '제임스맥어보이': 23769, '제치': 23770, '제크로무': 23771, '제프리': 23772, '제현': 23773, '제휴': 23774, '젝스키스': 23775, '젠더': 23776, '젠틀': 23777, '져간다': 23778, '져도': 23779, '져라': 23780, '져야한다': 23781, '젹': 23782, '졈': 23783, '졌어도': 23784, '졌었': 23785, '졌으며': 23786, '조각난': 23787, '조감독': 23788, '조곤조곤': 23789, '조교': 23790, '조급': 23791, '조나단': 23792, '조달환': 23793, '조델': 23794, '조두순': 23795, '조디악': 23796, '조련사': 23797, '조롱거리': 23798, '조르': 23799, '조미령': 23800, '조선일보': 23801, '조성하': 23802, '조세호': 23803, '조셉고든레빗': 23804, '조속': 23805, '조쉬더하멜': 23806, '조야': 23807, '조여': 23808, '조여드': 23809, '조영': 23810, '조영호': 23811, '조용원': 23812, '조운': 23813, '조위': 23814, '조이진': 23815, '조장혁': 23816, '조정래': 23817, '조촐': 23818, '조총련': 23819, '조페': 23820, '조필연': 23821, '족보': 23822, '족쇄': 23823, '존경심': 23824, '존레넌': 23825, '존카터': 23826, '존파브로': 23827, '졸랐': 23828, '졸려요': 23829, '졸린다': 23830, '종국': 23831, '종두': 23832, '종목': 23833, '종소리': 23834, '종속': 23835, '종신형': 23836, '종지부': 23837, '종철': 23838, '종초홍': 23839, '종형': 23840, '좋아해야': 23841, '좌편': 23842, '좌표': 23843, '죄고': 23844, '죄이': 23845, '죠지': 23846, '죨라': 23847, '주객': 23848, '주객전도': 23849, '주교': 23850, '주극': 23851, '주금': 23852, '주기철': 23853, '주둥이': 23854, '주라': 23855, '주랑': 23856, '주력': 23857, '주루': 23858, '주마': 23859, '주무': 23860, '주비리': 23861, '주사': 23862, '주사위': 23863, '주술사': 23864, '주안': 23865, '주어': 23866, '주영호': 23867, '주요소': 23868, '주워': 23869, '주웠': 23870, '주으러': 23871, '주이': 23872, '주작': 23873, '주저리': 23874, '주저리주저리': 23875, '주절': 23876, '주점': 23877, '주접떨': 23878, '주정뱅이': 23879, '주차장': 23880, '주찬옥': 23881, '주축': 23882, '주피터': 23883, '죽기': 23884, '죽순': 23885, '죽여요': 23886, '죽인다는': 23887, '준다거나': 23888, '준분': 23889, '준석': 23890, '준이치': 23891, '준장': 23892, '준형': 23893, '줄기차': 23894, '줄라면': 23895, '줄라이': 23896, '줄리엣비노쉬': 23897, '줄서': 23898, '줄어든': 23899, '줄여서': 23900, '줄인다': 23901, '줄일': 23902, '줄잡': 23903, '줄지': 23904, '중간고사': 23905, '중간광고': 23906, '중공업': 23907, '중구': 23908, '중국산': 23909, '중국집': 23910, '중반전': 23911, '중상': 23912, '중재': 23913, '중하': 23914, '줘야겠다': 23915, '줘야해': 23916, '줸좡': 23917, '쥐새끼': 23918, '쥐약': 23919, '쥐어뜯': 23920, '쥴리아': 23921, '쥼': 23922, '즈다': 23923, '즐거운지': 23924, '즐거워요': 23925, '증가': 23926, '증권': 23927, '증도': 23928, '증발': 23929, '증상': 23930, '지가': 23931, '지겨우': 23932, '지구촌': 23933, '지그래': 23934, '지끈거리': 23935, '지나데이비스': 23936, '지나왔': 23937, '지나칠': 23938, '지난날': 23939, '지녔': 23940, '지당': 23941, '지드래곤': 23942, '지라도': 23943, '지라치': 23944, '지러': 23945, '지력': 23946, '지른다고': 23947, '지를': 23948, '지많': 23949, '지면': 23950, '지물': 23951, '지발': 23952, '지배했': 23953, '지설': 23954, '지아장커': 23955, '지야': 23956, '지양': 23957, '지어': 23958, '지언정': 23959, '지여': 23960, '지영': 23961, '지영민': 23962, '지운다면': 23963, '지워졌': 23964, '지워진': 23965, '지윤': 23966, '지율': 23967, '지은': 23968, '지음': 23969, '지정': 23970, '지젤': 23971, '지젤번천': 23972, '지질': 23973, '지쳐서': 23974, '지칭': 23975, '지켜낸': 23976, '지켜야지': 23977, '지켜준다는': 23978, '지킨다': 23979, '지킬라고': 23980, '지팡': 23981, '지함': 23982, '지혈': 23983, '직결': 23984, '직구': 23985, '직무': 23986, '직인': 23987, '진가상': 23988, '진가신': 23989, '진단': 23990, '진드기': 23991, '진명호': 23992, '진목승': 23993, '진식': 23994, '진압': 23995, '진역': 23996, '진주만': 23997, '진주희': 23998, '진표': 23999, '진품': 24000, '진핵크만': 24001, '진했': 24002, '진행자': 24003, '진행형': 24004, '질감': 24005, '질급': 24006, '질끈': 24007, '질러라': 24008, '질러야': 24009, '질려서': 24010, '질렸': 24011, '질른': 24012, '질수록': 24013, '질정': 24014, '질척': 24015, '질퍽': 24016, '짊어진': 24017, '짐머': 24018, '집사람': 24019, '집어쳐라': 24020, '집어치워': 24021, '집필': 24022, '집합체': 24023, '집행': 24024, '집행자': 24025, '징가': 24026, '징그러웠': 24027, '징리': 24028, '징역': 24029, '징집': 24030, '징징댈': 24031, '징하': 24032, '짘ㅋㅋㅋ': 24033, '짜가리': 24034, '짜깁기': 24035, '짜리몽땅': 24036, '짜여졌': 24037, '짜인': 24038, '짜잔': 24039, '짜장': 24040, '짜지': 24041, '짝짝짝': 24042, '짤렸': 24043, '째지': 24044, '쨔': 24045, '쩖': 24046, '쩨': 24047, '쪼그려': 24048, '쪽팔렸': 24049, '쪾빠리': 24050, '쫄깃쫄깃': 24051, '쫓겨': 24052, '쫓아다닐': 24053, '쫓아오': 24054, '쫘': 24055, '쭝국': 24056, '쯔요시': 24057, '쯥': 24058, '쯪': 24059, '찌들린': 24060, '찌른': 24061, '찌릿': 24062, '찌이': 24063, '찌푸려졌': 24064, '찌푸릴': 24065, '찍소리': 24066, '찍혀': 24067, '찍힌': 24068, '찔끔찔끔': 24069, '찔러서': 24070, '찔려서': 24071, '찔렸': 24072, '찔린': 24073, '찡그리': 24074, '찡찡': 24075, '찢어진다': 24076, '찢어질': 24077, '차가우': 24078, '차근차근': 24079, '차렸': 24080, '차린다': 24081, '차비': 24082, '차승': 24083, '차심': 24084, '차오른다': 24085, '차인': 24086, '차진': 24087, '차차': 24088, '차치': 24089, '차해원': 24090, '차형사': 24091, '착시': 24092, '착한데': 24093, '착함': 24094, '찬드': 24095, '찬란히': 24096, '찬송가': 24097, '찰흙': 24098, '참뜻': 24099, '참말': 24100, '참석': 24101, '찻': 24102, '찼었': 24103, '창가': 24104, '창궐': 24105, '창작력': 24106, '창조성': 24107, '찾아와서': 24108, '찾아왔으면': 24109, '채경': 24110, '채령': 24111, '채린': 24112, '채림': 24113, '채연희': 24114, '채영': 24115, '채운다': 24116, '채워도': 24117, '채워서': 24118, '채워졌': 24119, '채탁연': 24120, '책방': 24121, '책임져': 24122, '책장': 24123, '챈': 24124, '챌린저': 24125, '챗': 24126, '챙겨라': 24127, '챙겨서': 24128, '챙긴다': 24129, '처다보': 24130, '처막': 24131, '처묵': 24132, '처바르': 24133, '처박': 24134, '처방': 24135, '처용': 24136, '처자': 24137, '처자식': 24138, '처절히': 24139, '처진': 24140, '처진다': 24141, '척박': 24142, '척척': 24143, '천기': 24144, '천룡팔부': 24145, '천신만고': 24146, '천옌시': 24147, '천은경': 24148, '천작': 24149, '천장': 24150, '천조': 24151, '천주교': 24152, '천진': 24153, '천하장사': 24154, '철거민': 24155, '철드는': 24156, '철렁': 24157, '철컹철컹': 24158, '철판': 24159, '첩보전': 24160, '첩첩산중': 24161, '첫날밤': 24162, '첫판': 24163, '첬네': 24164, '청년기': 24165, '청사진': 24166, '청순가련': 24167, '청아': 24168, '청옥': 24169, '청자': 24170, '청장': 24171, '체로': 24172, '체벌': 24173, '체위': 24174, '체이싱': 24175, '체질': 24176, '체취': 24177, '체코': 24178, '쳇바퀴': 24179, '쳐놨': 24180, '쳐다볼': 24181, '쳐들': 24182, '쳐들어왔': 24183, '쳐줄': 24184, '쳣': 24185, '초극': 24186, '초록물고기': 24187, '초롱초롱': 24188, '초상화': 24189, '초속': 24190, '초스피드': 24191, '초신성': 24192, '초치': 24193, '초토화': 24194, '촉발': 24195, '촉촉': 24196, '촌구석': 24197, '촌극': 24198, '촌놈': 24199, '촌뜨기': 24200, '촛': 24201, '총감독': 24202, '총망라': 24203, '총살': 24204, '총알받이': 24205, '총장': 24206, '총제': 24207, '총집': 24208, '총천연색': 24209, '총합': 24210, '최고조': 24211, '최곤': 24212, '최다': 24213, '최대치': 24214, '최미자': 24215, '최세용': 24216, '최수영': 24217, '최윤소': 24218, '최은희': 24219, '최정원': 24220, '최지연': 24221, '최철호': 24222, '최홍만': 24223, '최화정': 24224, '최효종': 24225, '쵝오네요': 24226, '쵝오의': 24227, '쵼나': 24228, '추녀': 24229, '추론': 24230, '추위': 24231, '추임': 24232, '추정': 24233, '추진력': 24234, '추천요': 24235, '추태': 24236, '추해': 24237, '추행': 24238, '축내': 24239, '축약': 24240, '춘다고': 24241, '춘추': 24242, '춘향뎐': 24243, '출렁이': 24244, '출발선': 24245, '출세욕': 24246, '출소': 24247, '출애굽기': 24248, '출입구': 24249, '출진': 24250, '출타': 24251, '충남': 24252, '충청도': 24253, '충혈': 24254, '췟': 24255, '취득': 24256, '취조': 24257, '취학전': 24258, '취화': 24259, '췬': 24260, '츠카사': 24261, '측정': 24262, '츰': 24263, '치고는': 24264, '치러': 24265, '치렁치렁': 24266, '치뤄야': 24267, '치마': 24268, '치민': 24269, '치바': 24270, '치사량': 24271, '치아': 24272, '치여': 24273, '치와와': 24274, '치우쳐서': 24275, '치워라': 24276, '치켜': 24277, '치토': 24278, '칙': 24279, '칙스': 24280, '친가': 24281, '친동생': 24282, '친모': 24283, '친밀': 24284, '친부모': 24285, '친영': 24286, '친절히': 24287, '친하': 24288, '친화': 24289, '칠려고': 24290, '칠봉': 24291, '침략자': 24292, '침치': 24293, '침해': 24294, '칩시다': 24295, '칫': 24296, '칭기즈칸': 24297, '칭할': 24298, '칭호': 24299, '카니': 24300, '카도카와': 24301, '카라': 24302, '카라스': 24303, '카렐': 24304, '카르마': 24305, '카르멘': 24306, '카르페': 24307, '카를로': 24308, '카리타': 24309, '카린': 24310, '카비젤': 24311, '카세료': 24312, '카야': 24313, '카우': 24314, '카우리스마키': 24315, '카우프만': 24316, '카우프먼': 24317, '카운트': 24318, '카탈레': 24319, '카펠라': 24320, '카프리': 24321, '칸노': 24322, '칼랑': 24323, '칼로리': 24324, '캄': 24325, '캉': 24326, '캐논': 24327, '캐머런': 24328, '캐쉬': 24329, '캐슬린': 24330, '캘': 24331, '캠벨': 24332, '캡슐': 24333, '캣츠': 24334, '캭': 24335, '커넥트': 24336, '커뮤니케이션': 24337, '커밍': 24338, '커밍아웃': 24339, '커스틴': 24340, '커진다': 24341, '커트러셀': 24342, '커트코베인': 24343, '커튼': 24344, '커티스': 24345, '커피숍': 24346, '컬러풀': 24347, '컬렉션': 24348, '컴팩트': 24349, '컵라면': 24350, '케니지': 24351, '케시베이츠': 24352, '케이드': 24353, '케이트베킨세일': 24354, '케이트블란쳇': 24355, '케이트허드슨': 24356, '케이티홈즈': 24357, '켄드릭': 24358, '켄조': 24359, '켄트': 24360, '켈': 24361, '켈리후': 24362, '코걸이': 24363, '코딱지': 24364, '코라': 24365, '코러스': 24366, '코로': 24367, '코리': 24368, '코메': 24369, '코메다': 24370, '코스모': 24371, '코아': 24372, '코언': 24373, '코트': 24374, '콘셉트': 24375, '콘텐츠': 24376, '콜럼버스': 24377, '콜롬비': 24378, '콜리야': 24379, '콧구멍': 24380, '콩트': 24381, '콸콸': 24382, '쿠로': 24383, '쿠르즈': 24384, '쿠릴렌코': 24385, '쿠마': 24386, '쿠시': 24387, '쿠아론': 24388, '쿠엔틴타란티노': 24389, '쿠쿠': 24390, '쿨러닝': 24391, '쿨쿨': 24392, '쿨하': 24393, '쿰': 24394, '쿵쾅': 24395, '퀀시': 24396, '퀄을': 24397, '퀄이': 24398, '퀴야': 24399, '퀵실버': 24400, '퀸카': 24401, '퀸틴': 24402, '큐티': 24403, '크라잉넛': 24404, '크랭크': 24405, '크로': 24406, '크로울리': 24407, '크로즈': 24408, '크로포드': 24409, '크리스터커': 24410, '크리스텔': 24411, '크리스토': 24412, '크리스토퍼리브': 24413, '크리스토프': 24414, '크메르': 24415, '큰데': 24416, '큰딸': 24417, '큰소리': 24418, '큰소리치': 24419, '큰일나': 24420, '큰코다치': 24421, '클라리넷': 24422, '클램프': 24423, '클러치': 24424, '클로드': 24425, '클로스': 24426, '클로징': 24427, '킄': 24428, '키드캅': 24429, '키득키득': 24430, '키리시마': 24431, '키무라': 24432, '키쓰': 24433, '키에누': 24434, '키즈': 24435, '키코': 24436, '키타노': 24437, '키타무라': 24438, '키팅': 24439, '키호테': 24440, '킥킥': 24441, '킨다': 24442, '킨스키': 24443, '킬리': 24444, '킬미': 24445, '킹카': 24446, '타가': 24447, '타계': 24448, '타깃': 24449, '타다노부': 24450, '타락천사': 24451, '타린': 24452, '타마키히로시': 24453, '타베': 24454, '타부': 24455, '타블로이드': 24456, '타석': 24457, '타오': 24458, '타운': 24459, '타원': 24460, '타이슨': 24461, '타이어': 24462, '타이완': 24463, '타일러페리': 24464, '타쿠야': 24465, '타티': 24466, '탄다': 24467, '탈렌트': 24468, '탈리': 24469, '탈바꿈': 24470, '탈의': 24471, '탈퇴': 24472, '탈해': 24473, '탐닉': 24474, '탕수육': 24475, '탕진': 24476, '태국어': 24477, '태백산맥': 24478, '태수': 24479, '태영': 24480, '태울': 24481, '태웅': 24482, '태종': 24483, '태후': 24484, '택트': 24485, '택할': 24486, '탬': 24487, '탱자': 24488, '터다': 24489, '터뜨릴': 24490, '터보': 24491, '터준': 24492, '터트려': 24493, '터트렸': 24494, '턱시도': 24495, '턴데': 24496, '털끝': 24497, '털털': 24498, '텃세': 24499, '테니': 24500, '테라': 24501, '테러단': 24502, '테리': 24503, '테리길리엄': 24504, '테오도르': 24505, '테이트': 24506, '테크': 24507, '텐트': 24508, '텔러': 24509, '토가시': 24510, '토글': 24511, '토니스콧': 24512, '토달': 24513, '토드': 24514, '토막내': 24515, '토메이': 24516, '토미노': 24517, '토이스': 24518, '토익': 24519, '토일': 24520, '토종': 24521, '토착': 24522, '토키': 24523, '토킹': 24524, '토탈': 24525, '토핑': 24526, '토해': 24527, '토했': 24528, '톡': 24529, '톰베린저': 24530, '통달': 24531, '통렬': 24532, '통신': 24533, '통탄': 24534, '통할까': 24535, '퇴역': 24536, '퇴학': 24537, '퇴행': 24538, '투견': 24539, '투라': 24540, '투리': 24541, '투모': 24542, '투병': 24543, '투성인': 24544, '투정': 24545, '투하': 24546, '퉷퉷': 24547, '튀어나온': 24548, '튀어나올': 24549, '튄다': 24550, '튈지': 24551, '트래비스': 24552, '트랜드': 24553, '트랜스': 24554, '트레이닝': 24555, '트레져': 24556, '트론': 24557, '트름': 24558, '트였': 24559, '트위스트김': 24560, '트위터': 24561, '트이': 24562, '특': 24563, '특강': 24564, '특출': 24565, '특출나': 24566, '특희': 24567, '튼실': 24568, '틀기': 24569, '틀려요': 24570, '틈타': 24571, '틈틈히': 24572, '티거': 24573, '티란': 24574, '티렉스': 24575, '티백': 24576, '티아': 24577, '티어': 24578, '티오': 24579, '티즈': 24580, '티티': 24581, '틴탑': 24582, '틴토브라스': 24583, '틴틴': 24584, '틸리': 24585, '팀로빈스': 24586, '팀워크': 24587, '파가': 24588, '파가니니': 24589, '파고든': 24590, '파극': 24591, '파노라마': 24592, '파니': 24593, '파닥파닥': 24594, '파도치': 24595, '파랗': 24596, '파레': 24597, '파병': 24598, '파수': 24599, '파식': 24600, '파이란': 24601, '파이어': 24602, '파킨슨병': 24603, '파타고니아': 24604, '파프리카': 24605, '판가름': 24606, '판대': 24607, '판의': 24608, '판이': 24609, '팔릴': 24610, '팔자': 24611, '팝업': 24612, '팠': 24613, '팥': 24614, '패권': 24615, '패딩턴': 24616, '패러': 24617, '패리스': 24618, '패스벤더': 24619, '패스트푸드': 24620, '패키지': 24621, '패킷': 24622, '패트리': 24623, '패티': 24624, '패티슨': 24625, '팩스톤': 24626, '팼': 24627, '퍼니셔': 24628, '퍼뜩': 24629, '퍼랜드': 24630, '퍼졌': 24631, '퍽퍽': 24632, '펄펄': 24633, '펏음': 24634, '펑키': 24635, '페': 24636, '페기': 24637, '페데리코': 24638, '페덱스': 24639, '페리': 24640, '페북': 24641, '페스티벌': 24642, '페시': 24643, '페어': 24644, '페이스북': 24645, '페이스풀': 24646, '페지': 24647, '페티쉬': 24648, '펜더': 24649, '펜싱': 24650, '편애': 24651, '편집자': 24652, '편차': 24653, '편함': 24654, '편했': 24655, '펼': 24656, '펼쳐집니다': 24657, '펼쳤': 24658, '펼친': 24659, '펼칠': 24660, '평과': 24661, '평수': 24662, '평일': 24663, '평판': 24664, '평할': 24665, '평해': 24666, '평했': 24667, '폐소': 24668, '폐쇄성': 24669, '폐하': 24670, '포덕': 24671, '포루': 24672, '포머': 24673, '포식자': 24674, '포영': 24675, '포옹': 24676, '포유': 24677, '포주': 24678, '포카': 24679, '포커페이스': 24680, '포탈': 24681, '포터': 24682, '포화': 24683, '폭력배': 24684, '폭렬': 24685, '폭삭': 24686, '폭신': 24687, '폭포수': 24688, '폴뉴먼': 24689, '폴란드': 24690, '폴리': 24691, '폴버호벤': 24692, '폴토마스앤더슨': 24693, '폴포츠': 24694, '퐁당': 24695, '표나': 24696, '표류': 24697, '표준어': 24698, '표창': 24699, '표한': 24700, '표현법': 24701, '푸념': 24702, '푸드': 24703, '푸시': 24704, '푼수': 24705, '풀린': 24706, '풀림': 24707, '풀어낸다': 24708, '풀어냄': 24709, '풀어놨': 24710, '품위': 24711, '품작': 24712, '풍겨': 24713, '풍년': 24714, '퓨처라마': 24715, '퓨터': 24716, '퓬': 24717, '퓽': 24718, '프라다': 24719, '프라이멀': 24720, '프라이즈': 24721, '프라하': 24722, '프랑소와즈': 24723, '프래': 24724, '프레드': 24725, '프레디하이모어': 24726, '프로게이머': 24727, '프로도': 24728, '프로방스': 24729, '프로이드': 24730, '프로파일러': 24731, '프로파일링': 24732, '프로페셔널': 24733, '프로펠러': 24734, '프로포폴': 24735, '프론': 24736, '프롤로그': 24737, '프롬': 24738, '프루프': 24739, '프리오': 24740, '프리즈': 24741, '프리티': 24742, '프릭스': 24743, '프린세스': 24744, '프린스': 24745, '플라이': 24746, '플래시': 24747, '플래시맨': 24748, '플레이어': 24749, '플레인': 24750, '플로': 24751, '피고': 24752, '피기': 24753, '피노토': 24754, '피도': 24755, '피땀': 24756, '피력': 24757, '피로연': 24758, '피리': 24759, '피바다': 24760, '피보': 24761, '피비케이츠': 24762, '피아': 24763, '피아제': 24764, '피알': 24765, '피어': 24766, '피어난다': 24767, '피어스브로스넌': 24768, '피오나': 24769, '피운': 24770, '피의자': 24771, '피츠제럴드': 24772, '피카': 24773, '피카츄': 24774, '피토': 24775, '피파': 24776, '피프': 24777, '피했': 24778, '픽픽': 24779, '필도': 24780, '필라': 24781, '필로': 24782, '필링': 24783, '필승': 24784, '필터링': 24785, '필하모닉': 24786, '핏': 24787, '하기야': 24788, '하나다': 24789, '하노': 24790, '하들리': 24791, '하렘': 24792, '하렘물': 24793, '하루빨리': 24794, '하모': 24795, '하바나': 24796, '하버드': 24797, '하사': 24798, '하산': 24799, '하삼': 24800, '하얀색': 24801, '하우어': 24802, '하울링': 24803, '하유': 24804, '하이랜더': 24805, '하이바라': 24806, '하이브리드': 24807, '하이스쿨': 24808, '하이힐': 24809, '하정': 24810, '하주': 24811, '하지': 24812, '하쿠나': 24813, '하하핫': 24814, '하향세': 24815, '하휘동': 24816, '학번': 24817, '학부': 24818, '한거': 24819, '한겨레': 24820, '한경': 24821, '한곡': 24822, '한다구요': 24823, '한다지': 24824, '한답니다': 24825, '한당': 24826, '한드': 24827, '한새': 24828, '한선': 24829, '한성': 24830, '한세경': 24831, '한올': 24832, '한우': 24833, '한인': 24834, '한전': 24835, '한족': 24836, '한지혜': 24837, '한진': 24838, '한철': 24839, '한큐': 24840, '한탄': 24841, '한평생': 24842, '한풀': 24843, '할걸': 24844, '할로우': 24845, '할애': 24846, '함은정': 24847, '함인지': 24848, '합디다': 24849, '합류': 24850, '합법화': 24851, '합의': 24852, '합쳐야': 24853, '합쳤': 24854, '핫바지': 24855, '핫산': 24856, '항공': 24857, '항마': 24858, '항목': 24859, '항변': 24860, '항암제': 24861, '항쟁': 24862, '핰핰핰': 24863, '핳': 24864, '핳ㅎ': 24865, '해괴': 24866, '해냄': 24867, '해논': 24868, '해놔서': 24869, '해두': 24870, '해드림': 24871, '해래': 24872, '해버려서': 24873, '해변가': 24874, '해부학': 24875, '해서든': 24876, '해서웨이': 24877, '해야겠단': 24878, '해야할까': 24879, '해어': 24880, '해일': 24881, '해자': 24882, '해저': 24883, '해적질': 24884, '해져라': 24885, '해져버린': 24886, '해졌으면': 24887, '해준다면': 24888, '해줍니다': 24889, '해줍시다': 24890, '해줘': 24891, '해진다는': 24892, '해커': 24893, '해코지': 24894, '해킹': 24895, '해탈': 24896, '해해': 24897, '해했': 24898, '핵전쟁': 24899, '핵크만': 24900, '핸섬': 24901, '햄스': 24902, '햅': 24903, '햅번': 24904, '햇볕': 24905, '했다': 24906, '행방불명': 24907, '행복감': 24908, '행적': 24909, '행크': 24910, '행할': 24911, '향후': 24912, '허걱': 24913, '허겁지겁': 24914, '허관걸': 24915, '허구헌': 24916, '허그': 24917, '허덕이': 24918, '허물': 24919, '허벅지': 24920, '허약': 24921, '허억': 24922, '허우샤오시엔': 24923, '허우적허우적': 24924, '허투루': 24925, '헌납': 24926, '험기': 24927, '험프리': 24928, '험한': 24929, '헛갈리': 24930, '헛것': 24931, '헛발': 24932, '헛짓': 24933, '헝클어진': 24934, '헠헠헠': 24935, '헤더그레이엄': 24936, '헤드폰': 24937, '헤르미온느': 24938, '헤매이': 24939, '헤비메탈': 24940, '헤어져야': 24941, '헤쳐나갈': 24942, '헤프': 24943, '헤헤헤헤': 24944, '헨드': 24945, '헬렌켈러': 24946, '헬리콥터': 24947, '헬싱': 24948, '헷갈렸': 24949, '현격': 24950, '현대물': 24951, '현대전': 24952, '현대차': 24953, '현도': 24954, '현미경': 24955, '현상계': 24956, '현수': 24957, '현시': 24958, '현시대': 24959, '현실주의자': 24960, '현아': 24961, '현암': 24962, '현정': 24963, '혈': 24964, '혈기': 24965, '혈연': 24966, '혐유영': 24967, '혐의': 24968, '협판': 24969, '형평': 24970, '혜미': 24971, '혜선': 24972, '호남': 24973, '호노카': 24974, '호로': 24975, '호빗호빗': 24976, '호사': 24977, '호소다마모루': 24978, '호스트': 24979, '호의': 24980, '호적': 24981, '호크아이': 24982, '호탕': 24983, '호파': 24984, '호호호': 24985, '호환': 24986, '혹여': 24987, '혹우': 24988, '혼선': 24989, '혼수': 24990, '혼연일체': 24991, '혼인': 24992, '혼잣말': 24993, '혼재': 24994, '혼전': 24995, '홀려': 24996, '홀려서': 24997, '홀리데이': 24998, '홀리헌터': 24999, '홀린': 25000, '홈피': 25001})\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = False,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        # lengths need to be on CPU!\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), enforce_sorted=False, batch_first=True)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)        \n",
    "        #unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, we'll create an instance of our RNN class, with the new parameters and arguments for the number of layers, bidirectionality and dropout probability.\n",
    "\n",
    "To ensure the pre-trained vectors can be loaded into the model, the `EMBEDDING_DIM` must be equal to that of the pre-trained GloVe vectors loaded earlier.\n",
    "\n",
    "We get our pad token index from the vocabulary, getting the actual string representing the pad token from the field's `pad_token` attribute, which is `<pad>` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 4\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The model has 7,964,777 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to training the model.\n",
    "\n",
    "The only change we'll make here is changing the optimizer from `SGD` to `Adam`. SGD updates all parameters with the same learning rate and choosing this learning rate can be tricky. `Adam` adapts the learning rate for each parameter, giving parameters that are updated more frequently lower learning rates and parameters that are updated infrequently higher learning rates. More information about `Adam` (and other optimizers) can be found [here](http://ruder.io/optimizing-gradient-descent/index.html).\n",
    "\n",
    "To change `SGD` to `Adam`, we simply change `optim.SGD` to `optim.Adam`, also note how we do not have to provide an initial learning rate for Adam as PyTorch specifies a sensibile default initial learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the steps for training the model are unchanged.\n",
    "\n",
    "We define the criterion and place the model and criterion on the GPU (if available)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function for training our model. \n",
    "\n",
    "As we have set `include_lengths = True`, our `batch.text` is now a tuple with the first element being the numericalized tensor and the second element being the actual lengths of each sequence. We separate these into their own variables, `text` and `text_lengths`, before passing them to the model.\n",
    "\n",
    "**Note**: as we are now using dropout, we must remember to use `model.train()` to ensure the dropout is \"turned on\" while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "\n",
    "        # print(text)\n",
    "        # print(text_lengths)\n",
    "        \n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a function for testing our model, again remembering to separate `batch.text`.\n",
    "\n",
    "**Note**: as we are now using dropout, we must remember to use `model.eval()` to ensure the dropout is \"turned off\" while evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also create a nice function to tell us how long our epochs are taking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train our model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-c1b298b1eeea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-125-9f781e211e87>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# print(text_lengths)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/momo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-119-946124e49745>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, text_lengths)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# lengths need to be on CPU!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mpacked_embedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mpacked_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_embedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/momo/lib/python3.7/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_packed_sequence_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and get our new and vastly improved test accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Loss: 0.415 | Test Acc: 82.02%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut2-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input\n",
    "\n",
    "We can now use our model to predict the sentiment of any sentence we give it. As it has been trained on movie reviews, the sentences provided should also be movie reviews.\n",
    "\n",
    "When using a model for inference it should always be in evaluation mode. If this tutorial is followed step-by-step then it should already be in evaluation mode (from doing `evaluate` on the test set), however we explicitly set it to avoid any risk.\n",
    "\n",
    "Our `predict_sentiment` function does a few things:\n",
    "- sets the model to evaluation mode\n",
    "- tokenizes the sentence, i.e. splits it from a raw string into a list of tokens\n",
    "- indexes the tokens by converting them into their integer representation from our vocabulary\n",
    "- gets the length of our sequence\n",
    "- converts the indexes, which are a Python list into a PyTorch tensor\n",
    "- add a batch dimension by `unsqueeze`ing \n",
    "- converts the length into a tensor\n",
    "- squashes the output prediction from a real number between 0 and 1 with the `sigmoid` function\n",
    "- converts the tensor holding a single value into an integer with the `item()` method\n",
    "\n",
    "We are expecting reviews with a negative sentiment to return a value close to 0 and positive reviews to return a value close to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example negative review..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.08191801607608795"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example positive review..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9730545878410339"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is great\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "We've now built a decent sentiment analysis model for movie reviews! In the next notebook we'll implement a model that gets comparable accuracy with far fewer parameters and trains much, much faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd0f8af005c6536e801c34ed1a329b6361f501640f22aff193c57cfc4ad0e1dbb64",
   "display_name": "Python 3.7.10 64-bit ('momo': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}